import asyncio
import hashlib
import torch
import os
import time
import requests
import re
import json
from typing import List, Dict, Any, Optional, Union, Tuple
from dotenv import load_dotenv
from src.text_processing.ai.gigachat import rewrite_text_giga
from src.text_processing.ai.deepseek import rewrite_text_deepseek
from loguru import logger
from database.db import create_db_pool
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
from src.vk_function import remove_vk_links_but_keep_text

load_dotenv()

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
# –ï—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω sentence-transformers: pip install sentence-transformers
model = SentenceTransformer(os.getenv("LOCAL_BERT_VECTOR_MODEL_PATH"))  # –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –Ω–∞ –¥–∏—Å–∫
# model = SentenceTransformer("all-mpnet-base-v2") # –≤–µ–∫—Ç–æ—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 768
# model = SentenceTransformer("paraphrase-mpnet-base-v2")  # –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –º–æ–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º

# –ü–æ—Ä–æ–≥ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–µ–π
SEMANTIC_THRESHOLD = 0.95

# –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è AI (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
AI_DISABLED = False  # –ü–æ—Å—Ç–∞–≤—å True –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è AI

# --- AI Provider Switcher ---
def ai_provider():
    ai = os.getenv("AI_PROVIDER", "gigachat").lower()
    if ai == "deepseek":
        logger.info("AI-–ø—Ä–æ–≤–∞–π–¥–µ—Ä: DeepSeek")
        return rewrite_text_deepseek
    else:
        logger.info("AI-–ø—Ä–æ–≤–∞–π–¥–µ—Ä: GigaChat")
        return rewrite_text_giga

async def log_skipped_post(conn, new_post_url: str, reason: str,
                          hash_value: str = None, similar_post_url: str = None,
                          similarity: float = None, group_name: str = None,
                          raw_text: str = None):
    """
    –õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–π –ø–æ—Å—Ç –≤ —Ç–∞–±–ª–∏—Ü—É skipped_posts.
    """
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø–æ—Ö–æ–∂–µ–≥–æ –ø–æ—Å—Ç–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    similar_post_date = None
    if similar_post_url:
        try:
            similar_post_result = await conn.fetchrow(
                "SELECT created_at FROM posts WHERE original_post_url = $1", 
                similar_post_url
            )
            if similar_post_result:
                similar_post_date = similar_post_result['created_at']
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—É –ø–æ—Ö–æ–∂–µ–≥–æ –ø–æ—Å—Ç–∞: {e}")
    
    await conn.execute(
        """
        INSERT INTO skipped_posts (
            new_post_url, reason, hash, similar_post_url, similarity, group_name, raw_text, similar_post_date
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
        ON CONFLICT (new_post_url, reason, hash) DO NOTHING
        """,
        new_post_url, reason, hash_value, similar_post_url, similarity, group_name, raw_text, similar_post_date
    )


async def filter_by_hash(posts: List[dict], conn) -> tuple[List[dict], int]:  # –ù–æ–≤—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –ø–æ —Ö—ç—à—É (—Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏) —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.
    """
    unique_posts = []
    skipped = 0
    for post in posts:
        text = post.get("text", "").strip()
        if not text:
            skipped += 1
            continue
        hash_value = hashlib.sha256(text.encode("utf-8")).hexdigest()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–∞–∫–æ–π —Ö—ç—à –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        result = await conn.fetchrow("SELECT hash, original_post_url FROM posts WHERE hash = $1", hash_value)
        if result:
            # –•—ç—à —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            await log_skipped_post(
                conn,
                new_post_url=post.get("original_post_url"),
                reason="hash_duplicate",
                hash_value=hash_value,
                similar_post_url=result['original_post_url'],  # URL –ø–æ—Ö–æ–∂–µ–≥–æ –ø–æ—Å—Ç–∞
                group_name=post.get("group_name"),
                raw_text=text
            )
            skipped += 1
            continue
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö—ç—à –∫ –ø–æ—Å—Ç—É –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        post['hash'] = hash_value
        unique_posts.append(post)
    return unique_posts, skipped


async def filter_by_url(posts: List[dict], conn) -> tuple[List[dict], int]:
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É URL —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏.
    """
    unique_posts = []
    skipped = 0
    seen_urls = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è URL –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏
    
    for post in posts:
        url = post.get("original_post_url")
        group_name = post.get("group_name", "")
        
        if not url:
            unique_posts.append(post)
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏
        # url_key = f"{url}_{group_name}"  # –ë–†–ï–î!
        if url in seen_urls:
            await log_skipped_post(
                conn,
                new_post_url=url,
                reason="session_duplicate",
                similar_post_url=url,
                group_name=group_name,
                raw_text=post.get("text", "")
            )
            skipped += 1
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–∞–∫–æ–π URL –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        result = await conn.fetchrow("SELECT original_post_url FROM posts WHERE original_post_url = $1", url)
        if result:
            # URL —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            await log_skipped_post(
                conn,
                new_post_url=url,
                reason="url_duplicate",
                similar_post_url=result['original_post_url'],  # URL –ø–æ—Ö–æ–∂–µ–≥–æ –ø–æ—Å—Ç–∞
                group_name=group_name,
                raw_text=post.get("text", "")
            )
            skipped += 1
            continue
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ
        seen_urls.add(url)
        unique_posts.append(post)
    return unique_posts, skipped

def select_best_post_from_group(posts: List[dict]) -> dict:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –ø–æ—Å—Ç –∏–∑ –≥—Ä—É–ø–ø—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–µ–π.
    
    –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞):
    1. –ù–∞–ª–∏—á–∏–µ –º–µ–¥–∏–∞–∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–∫–∞—Ä—Ç–∏–Ω–∫–∏, –≤–∏–¥–µ–æ, GIF)
    2. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ (–±–æ–ª—å—à–µ = –ª—É—á—à–µ)
    3. –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫ –∏ –ø—Ä–µ–≤—å—é
    
    Args:
        posts: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤-–¥—É–±–ª–µ–π –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ
        
    Returns:
        dict: –õ—É—á—à–∏–π –ø–æ—Å—Ç –∏–∑ –≥—Ä—É–ø–ø—ã
    """
    if not posts:
        return None
    
    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ—Å—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
    if len(posts) == 1:
        return posts[0]
    
    best_post = posts[0]
    best_score = 0
    
    for post in posts:
        score = 0
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π 1: –ú–µ–¥–∏–∞–∫–æ–Ω—Ç–µ–Ω—Ç (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        media_urls = post.get("media_urls", [])
        video_urls = post.get("video_urls", [])
        gif_urls = post.get("gif_urls", [])
        
        if media_urls or video_urls or gif_urls:
            score += 1000  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –º–µ–¥–∏–∞–∫–æ–Ω—Ç–µ–Ω—Ç—É
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π 2: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞
        text = post.get("text", "")
        text_length = len(text.strip())
        score += text_length
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π 3: –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫ –∏ –ø—Ä–µ–≤—å—é
        link_preview = post.get("link_preview", {})
        if link_preview and link_preview.get("url"):
            score += 100
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π 4: –ù–∞–ª–∏—á–∏–µ –ø—Ä–µ–≤—å—é –∫–∞—Ä—Ç–∏–Ω–∫–∏ —É —Å—Å—ã–ª–∫–∏
        if link_preview and link_preview.get("photo_url"):
            score += 50
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π –ø–æ—Å—Ç, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∏–º–µ–µ—Ç –±–æ–ª—å—à–∏–π —Å–∫–æ—Ä
        if score > best_score:
            best_score = score
            best_post = post
    
    return best_post

async def filter_by_semantic(posts: List[dict], conn) -> tuple[List[dict], int]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.
    
    –ò–ó–ú–ï–ù–ï–ù–ò–ï: –¢–µ–ø–µ—Ä—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥—Ä—É–ø–ø—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–µ–π –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞.
    –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–∞ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Å—Ç–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –∏–∑ –≥—Ä—É–ø–ø—ã –¥—É–±–ª–µ–π.
    """
    unique_posts = []
    skipped = 0
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    db_vectors = await conn.fetch("SELECT vector_raw, original_post_url FROM posts WHERE vector_raw IS NOT NULL")
    db_embeddings = []
    db_urls = []
    if db_vectors:
        for row in db_vectors:
            if row['vector_raw']:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤–µ–∫—Ç–æ—Ä–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–Ω–∑–æ—Ä
                vector_str = row['vector_raw']
                # –£–±–∏—Ä–∞–µ–º —Å–∫–æ–±–∫–∏ –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç–æ–π
                # vector_list = [float(x) for x in vector_str.strip('[]').split(',')]  # –•—Ä—É–ø–∫–∞—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
                vector_list = json.loads(vector_str)  # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ
                db_embeddings.append(torch.tensor(vector_list))
                db_urls.append(row['original_post_url'])
        
        # –û–¢–õ–ê–î–ö–ê: –≤—ã–≤–æ–¥–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        logger.info(f"üîç –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(db_embeddings)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –±–∞–∑—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        # logger.info(f"üîç URL'—ã –≤ –±–∞–∑–µ: {db_urls}")
    
    # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã –∏ –Ω–∞—Ö–æ–¥–∏–º –≥—Ä—É–ø–ø—ã –¥—É–±–ª–µ–π
    processed_posts = []  # –ü–æ—Å—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏
    semantic_groups = []  # –ì—Ä—É–ø–ø—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–µ–π
    
    # –®–∞–≥ 1: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã –∏ —Å—á–∏—Ç–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã
    for post in posts:
        text = post.get("text", "").strip()
        if not text:
            skipped += 1
            continue
        
        emb = model.encode(text, convert_to_tensor=True, normalize_embeddings=True)
        post['vector_raw'] = emb.tolist()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        processed_posts.append(post)
    
    # –®–∞–≥ 2: –ù–∞—Ö–æ–¥–∏–º –≥—Ä—É–ø–ø—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–µ–π
    used_indices = set()
    
    for i, post in enumerate(processed_posts):
        if i in used_indices:
            continue
        
        # –°–æ–∑–¥–∞—ë–º –≥—Ä—É–ø–ø—É –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–æ—Å—Ç–∞
        current_group = [post]
        used_indices.add(i)
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –ø–æ—Å—Ç—ã —Å—Ä–µ–¥–∏ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è
        for j in range(i + 1, len(processed_posts)):
            if j in used_indices:
                continue
            # === –ó–ê–©–ò–¢–ê: –Ω–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ—Å—Ç —Å–∞–º —Å —Å–æ–±–æ–π ===
            if processed_posts[i].get("original_post_url") == processed_posts[j].get("original_post_url"):
                continue
            
            other_post = processed_posts[j]
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã
            emb1 = torch.tensor(post['vector_raw'])
            emb2 = torch.tensor(other_post['vector_raw'])
            similarity = float(util.cos_sim(emb1.unsqueeze(0), emb2.unsqueeze(0))[0][0])
            
            if similarity > SEMANTIC_THRESHOLD:
                # –ù–∞—à–ª–∏ –¥—É–±–ª—å - –¥–æ–±–∞–≤–ª—è–µ–º –≤ –≥—Ä—É–ø–ø—É
                current_group.append(other_post)
                used_indices.add(j)
                
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—É–±–ª—å –≤ —Ä–∞–º–∫–∞—Ö –∑–∞–ø—É—Å–∫–∞: {post.get('original_post_url')} <-> {other_post.get('original_post_url')} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f})")
        
        # –ï—Å–ª–∏ –≥—Ä—É–ø–ø–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ - —ç—Ç–æ –≥—Ä—É–ø–ø–∞ –¥—É–±–ª–µ–π
        if len(current_group) > 1:
            semantic_groups.append(current_group)
            logger.info(f"üîç –°–æ–∑–¥–∞–Ω–∞ –≥—Ä—É–ø–ø–∞ –∏–∑ {len(current_group)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–µ–π")
        else:
            # –û–¥–∏–Ω–æ—á–Ω—ã–π –ø–æ—Å—Ç - –¥–æ–±–∞–≤–ª—è–µ–º –≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
            unique_posts.append(post)
    
    # –®–∞–≥ 3: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—ã –¥—É–±–ª–µ–π
    for group in semantic_groups:
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –ø–æ—Å—Ç –∏–∑ –≥—Ä—É–ø–ø—ã
        best_post = select_best_post_from_group(group)
        
        if best_post:
            unique_posts.append(best_post)
            skipped += len(group) - 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å–µ, –∫—Ä–æ–º–µ –ª—É—á—à–µ–≥–æ
            
            logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω –ª—É—á—à–∏–π –ø–æ—Å—Ç –∏–∑ –≥—Ä—É–ø–ø—ã: {best_post.get('original_post_url')}")
            logger.info(f"üìä –ü—Ä–æ–ø—É—â–µ–Ω–æ {len(group) - 1} –¥—É–±–ª–µ–π –∏–∑ –≥—Ä—É–ø–ø—ã")
    
    # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –ø–æ—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –±–∞–∑–µ)
    final_unique_posts = []
    
    for post in unique_posts:
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        is_duplicate_with_db = False
        
        if db_embeddings:
            emb = torch.tensor(post['vector_raw'])
            db_batch = torch.stack(db_embeddings)
            similarities = util.cos_sim(emb, db_batch)[0]
            max_sim = float(similarities.max())
            
            if max_sim > SEMANTIC_THRESHOLD:
                # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
                max_idx = int(similarities.argmax())
                similar_url = db_urls[max_idx] if max_idx < len(db_urls) else None
                
                logger.info(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—É–±–ª—å —Å –±–∞–∑–æ–π: {post.get('original_post_url')} -> {similar_url} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {max_sim:.4f})")
                logger.info(f"üîç –¢–µ–∫—Å—Ç –Ω–æ–≤–æ–≥–æ –ø–æ—Å—Ç–∞: '{post.get('text', '')[:100]}...'")
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Ö–æ–∂–µ–≥–æ –ø–æ—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                similar_post_result = await conn.fetchrow("SELECT raw_text FROM posts WHERE original_post_url = $1", similar_url)
                if similar_post_result:
                    similar_text = similar_post_result['raw_text']
                    logger.info(f"üîç –¢–µ–∫—Å—Ç –ø–æ—Ö–æ–∂–µ–≥–æ –ø–æ—Å—Ç–∞: '{similar_text[:100]}...'")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—É–±–ª—å
                await log_skipped_post(
                    conn,
                    new_post_url=post.get("original_post_url"),
                    reason="semantic_duplicate",
                    similarity=max_sim,
                    similar_post_url=similar_url,
                    group_name=post.get("group_name"),
                    raw_text=post.get("text", "")
                )
                skipped += 1
                is_duplicate_with_db = True
        
        if not is_duplicate_with_db:
            final_unique_posts.append(post)
    
    return final_unique_posts, skipped


async def filter_by_video_size(posts: List[dict], conn) -> tuple[List[dict], int]:
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –ø–æ —Ä–∞–∑–º–µ—Ä—É –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤.
    –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç –ø–æ—Å—Ç—ã —Å –≤–∏–¥–µ–æ –±–æ–ª—å—à–µ 250MB.
    """
    unique_posts = []
    skipped = 0
    
    for post in posts:
        video_urls = post.get("video_urls", [])
        
        if not video_urls:
            # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∏–¥–µ–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
            unique_posts.append(post)
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–µ–æ
        oversized_videos = []
        for video_url in video_urls:
            try:
                # # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ
                # from src.vk_video_downloader import get_vk_video_info
                # video_info = get_vk_video_info(video_url)
                
                # if video_info:
                #     # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ MB (–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è)
                #     file_size = video_info.get("filesize") or video_info.get("filesize_approx") or 0
                #     file_size_mb = file_size / (1024 * 1024) if file_size else 0
                    
                #     if file_size_mb > 250:
                #         oversized_videos.append(f"{video_url} ({file_size_mb:.2f}MB)")
                #         logger.warning(f"‚ö†Ô∏è –í–∏–¥–µ–æ –±–æ–ª—å—à–µ 250MB ({file_size_mb:.2f}MB): {video_url}")
                #     elif file_size_mb == 0:
                #         # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω, –ª–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                #         logger.info(f"üìπ –†–∞–∑–º–µ—Ä –≤–∏–¥–µ–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω: {video_url}")
                #         # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É, –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω
                #         continue
                #
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤–∏–¥–µ–æ –Ω–æ–≤—ã–º —Å–ø–æ—Å–æ–±–æ–º, —á–µ—Ä–µ–∑ –Ω–∞—á–∞–ª–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞.
                from temp import get_vk_video_size
                size = get_vk_video_size(video_url)
                if size > 150:
                    oversized_videos.append(f"{video_url} ({size:.2f}MB)")
                    logger.warning(f"‚ö†Ô∏è –í–∏–¥–µ–æ –±–æ–ª—å—à–µ 150MB ({size:.2f}MB): {video_url}")
                elif size == 0:
                    # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω, –ª–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    logger.info(f"üìπ –†–∞–∑–º–µ—Ä –≤–∏–¥–µ–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω: {video_url}")
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É, –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω
                    continue

                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–∞–∑–º–µ—Ä–∞ –≤–∏–¥–µ–æ {video_url}: {e}")
                # –ü—Ä–∏ –æ—à–∏–±–∫–µ —Å—á–∏—Ç–∞–µ–º –≤–∏–¥–µ–æ –±–æ–ª—å—à–∏–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                oversized_videos.append(f"{video_url} (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏)")
        
        if oversized_videos:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –±–æ–ª—å—à–∏–µ –≤–∏–¥–µ–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–µ—Å—å –ø–æ—Å—Ç
            await log_skipped_post(
                conn,
                new_post_url=post.get("original_post_url"),
                reason="video_too_large",
                group_name=post.get("group_name"),
                raw_text=post.get("text", "")
            )
            skipped += 1
            continue
        
        unique_posts.append(post)
    
    return unique_posts, skipped

async def rewrite_posts_ai(posts: List[dict], rewrite_func) -> tuple[List[dict], int]:  # –ù–æ–≤—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
    """
    –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ AI-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (GigaChat/DeepSeek).
    
    Args:
        posts: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–ª–µ–º "text" (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç)
        rewrite_func: –§—É–Ω–∫—Ü–∏—è AI-–ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è (async –∏–ª–∏ sync)
    
    Returns:
        tuple: (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ_–ø–æ—Å—Ç—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_–ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã—Ö)
        
    –õ–æ–≥–∏–∫–∞:
        1. –ï—Å–ª–∏ AI_DISABLED=True - –∫–æ–ø–∏—Ä—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ rewritten_text
        2. –ò–Ω–∞—á–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –≤ AI —á–µ—Ä–µ–∑ rewrite_func
        3. –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–Ω–µ –ø—É—Å—Ç–æ–π, –∏–∑–º–µ–Ω–∏–ª—Å—è)
        4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ post["rewritten_text"] –∏ post["text"]
        5. –ü—Ä–∏ –æ—à–∏–±–∫–µ AI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª
        
    –ö–∞–∂–¥—ã–π –ø–æ—Å—Ç –ø–æ–ª—É—á–∞–µ—Ç:
        - post["rewritten_text"] - –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–¥–ª—è –ë–î)
        - post["text"] - —Ç–µ–∫—Å—Ç –¥–ª—è Telegram (–ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π –∏–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª)
    """
    logger.info(f"üìù –ë—É–¥–µ—Ç –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ AI...")
    rewritten_count = 0
    updated_posts = []
    for post in posts:
        original_text = post.get("text", "").strip()
        
        if AI_DISABLED:
            # –ó–ê–¢–´–ß–ö–ê: –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            post["rewritten_text"] = original_text
            # –î–ª—è Telegram –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            post["text"] = original_text
            rewritten_count += 1
            logger.info(f"üîß –ó–ê–¢–´–ß–ö–ê: —Ç–µ–∫—Å—Ç —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –¥–ª—è: {post.get('original_post_url')}")
        else:
            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥ AI
            rewritten_text = None
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π
                if asyncio.iscoroutinefunction(rewrite_func):
                    rewritten_text = await rewrite_func(original_text)
                else:
                    # –î–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.to_thread
                    rewritten_text = await asyncio.to_thread(rewrite_func, original_text)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ AI-–ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è: {e}")
            if rewritten_text and rewritten_text.strip() and rewritten_text.strip() != original_text:
                post["rewritten_text"] = rewritten_text.strip()
                # –î–ª—è Telegram –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                post["text"] = rewritten_text.strip()
                rewritten_count += 1
                logger.success(f"‚úÖ –¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω –¥–ª—è: {post.get('original_post_url')}")
            else:
                post["rewritten_text"] = original_text
                # –î–ª—è Telegram –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
                post["text"] = original_text
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è: {post.get('original_post_url')}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª.")
        
        updated_posts.append(post)
    return updated_posts, rewritten_count

async def save_to_db(posts: List[dict], pool) -> int:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (—Ç–∞–±–ª–∏—Ü–∞ posts).
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç INSERT INTO posts ...
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤.
    """
    inserted = 0
    async with pool.acquire() as conn:
        for post in posts:
            try:
                text = post.get("text", "").strip()
                rewritten_text = post.get("rewritten_text", "").strip()
                hash_value = post.get('hash') or hashlib.sha256(text.encode("utf-8")).hexdigest()
                original_post_url = post.get("original_post_url")
                group_name = post.get("group_name")
                post_date_str = post.get("post_date")
                try:
                    post_date = datetime.strptime(post_date_str, "%Y-%m-%d %H:%M:%S") if post_date_str else None
                except Exception:
                    post_date = None
                media_urls = post.get("media_urls", [])
                gif_urls = post.get("gif_urls", [])
                video_urls = post.get("video_urls", [])
                link_preview = post.get("link_preview") or {}
                link_preview_url = link_preview.get("url")
                link_preview_photo_url = link_preview.get("photo_url")

                # –í–µ–∫—Ç–æ—Ä –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                vector_raw = post.get('vector_raw')
                vector_raw_str = None
                if vector_raw:
                    vector_raw_str = "[" + ",".join(map(str, vector_raw)) + "]"
                
                # –í–µ–∫—Ç–æ—Ä –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                vector_rewritten = None
                vector_rewritten_str = None
                if rewritten_text and rewritten_text != text:
                    try:
                        rewritten_emb = model.encode(rewritten_text, normalize_embeddings=True)
                        vector_rewritten = rewritten_emb.tolist()
                        vector_rewritten_str = "[" + ",".join(map(str, vector_rewritten)) + "]"
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
                
                await conn.execute(
                    """
                    INSERT INTO posts (
                        hash, raw_text, rewritten_text, vector_raw, vector_rewritten,
                        original_post_url, group_name, post_date,
                        media_urls, gif_urls, video_urls, link_preview_url, link_preview_photo_url
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    ON CONFLICT (hash) DO NOTHING
                    """,
                    hash_value, text, rewritten_text, vector_raw_str, vector_rewritten_str,
                    original_post_url, group_name, post_date,
                    media_urls, gif_urls, video_urls, link_preview_url, link_preview_photo_url
                )
                inserted += 1
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø–æ—Å—Ç–∞ –≤ –±–∞–∑—É: {e}")
    logger.info(f"‚úÖ –í –±–∞–∑—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {inserted} –ø–æ—Å—Ç–æ–≤.")
    return inserted

async def process_posts(posts: List[dict], pool) -> tuple[Dict[str, Any], List[dict]]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–æ–≤.
    –¢–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –±–∞–∑—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂: (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, —Å–ø–∏—Å–æ–∫ –æ–¥–æ–±—Ä–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤)
    """
    total = len(posts)
    async with pool.acquire() as conn:
        posts, skipped_by_url = await filter_by_url(posts, conn)
        posts, skipped_by_hash = await filter_by_hash(posts, conn)
        posts, skipped_by_semantic = await filter_by_semantic(posts, conn)
        posts, skipped_by_size = await filter_by_video_size(posts, conn)
    
    rewrite_func = ai_provider()
    posts, rewritten = await rewrite_posts_ai(posts, rewrite_func)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ–¥–æ–±—Ä–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    approved_posts = posts.copy()

    # –ø–µ—Ä–µ–¥–∞—ë–º pool –≤–Ω—É—Ç—Ä—å
    inserted = await save_to_db(posts, pool)

    skipped = skipped_by_hash + skipped_by_url + skipped_by_semantic + skipped_by_size
    stats = {
        "total": total,
        "inserted": inserted,
        "skipped": skipped,
        "skipped_by_hash": skipped_by_hash,
        "skipped_by_url": skipped_by_url,
        "skipped_by_semantic": skipped_by_semantic,
        "skipped_by_size": skipped_by_size,
        "rewritten": rewritten,
        "errors": 0
    } 
    
    return stats, approved_posts

def get_vk_last_posts(
    access_token: str,
    group_names: List[str],
    count: int = 5,
    save_path: Optional[str] = None,
    delay_between_requests: float = 0.3,
) -> Tuple[List[Dict], Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ VK –≥—Ä—É–ø–ø —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏.
    
    Args:
        access_token: VK API —Ç–æ–∫–µ–Ω
        group_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –≥—Ä—É–ø–ø
        count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        delay_between_requests: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.3)
    """
    

    
    def get_group_info(access_token: str, group_name: str, api_version: str) -> dict:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–µ/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ —á–µ—Ä–µ–∑ utils.resolveScreenName.
        –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏: screen_name, —á–∏—Å–ª–æ–≤—ã–µ ID, id123456789.
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º utils.resolveScreenName –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            response = requests.get(
                "https://api.vk.com/method/utils.resolveScreenName",
                params={
                    "access_token": access_token,
                    "v": api_version,
                    "screen_name": group_name
                },
                timeout=10
            ).json()
            
            if "error" in response:
                raise Exception(f"VK API Error: {response['error']['error_msg']}")
            
            result = response["response"]
            if not result:
                raise Exception("Group not found")
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to resolve screen name for {group_name}: {str(e)}")
            raise
    
    api_url = "https://api.vk.com/method/"
    api_version = "5.199"
    all_posts = []
    stats = {
        "total_groups": len(group_names),
        "processed_groups": 0,
        "failed_groups": 0,
        "skipped_types": {},
        "total_posts": 0,
        "reposts": 0,
        "group_errors": {},  # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö –≥—Ä—É–ø–ø
    }

    for group in group_names:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–±—ä–µ–∫—Ç–∞ –¥–ª—è –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        try:
            group_info = get_group_info(access_token, group, api_version)
            object_type = group_info["type"]
            
            if object_type == "group":
                logger.info(f"üë• Fetching group data: {group}")
            elif object_type == "page":
                logger.info(f"üìÑ Fetching page data: {group}")
            elif object_type == "user":
                logger.info(f"üë§ Fetching user data: {group}")
            else:
                logger.info(f"üåê Fetching unknown data: {group}")
        except:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π —Ñ–æ—Ä–º–∞—Ç
            logger.info(f"üåê Fetching data: {group}")

        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–µ —á–µ—Ä–µ–∑ utils.resolveScreenName
            # group_info —É–∂–µ –ø–æ–ª—É—á–µ–Ω –≤—ã—à–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(delay_between_requests)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–±—ä–µ–∫—Ç–∞ –∏ –ø–æ–ª—É—á–∞–µ–º ID
            object_type = group_info["type"]  # "group" –∏–ª–∏ "user"
            object_id = group_info["object_id"]
            
            if object_type == "group" or object_type == "page":
                group_id = -object_id  # –î–ª—è –≥—Ä—É–ø–ø –∏ –ø—É–±–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π ID
                group_name = group  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è
            elif object_type == "user":
                group_id = object_id  # –î–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π ID
                group_name = group  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è
            else:
                logger.warning(f"[Group Error] {group}: Unknown object type: {object_type}")
                stats["failed_groups"] += 1
                stats["group_errors"][group] = f"Unknown object type: {object_type}"
                continue

            # –ó–∞–ø—Ä–æ—Å –ø–æ—Å—Ç–æ–≤ –≥—Ä—É–ø–ø—ã —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            wall_response = requests.get(
                f"{api_url}wall.get",
                params={
                    "access_token": access_token,
                    "v": api_version,
                    "owner_id": group_id,
                    "count": count
                },
                timeout=10  # –¢–∞–π–º–∞—É—Ç 10 —Å–µ–∫—É–Ω–¥
            ).json()
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(delay_between_requests)

            if "error" in wall_response:
                error_msg = wall_response['error']['error_msg']
                logger.warning(f"[Wall Error] {group}: {error_msg}")
                stats["failed_groups"] += 1
                stats["group_errors"][group] = error_msg
                continue

            stats["processed_groups"] += 1

            for post in wall_response["response"]["items"]:
                stats["total_posts"] += 1

                post_id = post["id"]
                post_url = f"https://vk.com/wall{group_id}_{post_id}"

                post_date = datetime.utcfromtimestamp(post["date"]).strftime('%Y-%m-%d %H:%M:%S')
                is_repost = "copy_history" in post
                if is_repost:
                    stats["reposts"] += 1

                media_urls = []
                video_urls = []
                skipped_types = []
                link_preview = None
                doc_urls = []
                gif_urls = []

                for attach in post.get("attachments", []):
                    att_type = attach.get("type")
                    if att_type == "photo":
                        sizes = attach["photo"]["sizes"]
                        max_photo = max(sizes, key=lambda s: s["width"] * s["height"])
                        media_urls.append(max_photo["url"])
                    elif att_type == "video":
                        video = attach.get("video", {})
                        owner_id = video.get("owner_id")
                        video_id = video.get("id")
                        if owner_id is not None and video_id is not None:
                            video_url = f"https://vk.com/video{owner_id}_{video_id}"
                            video_urls.append(video_url)
                    elif att_type == "link":
                        link = attach.get("link", {})
                        url = link.get("url")
                        photo_url = None
                        photo = link.get("photo")
                        if photo:
                            sizes = photo.get("sizes", [])
                            if sizes:
                                photo_url = sizes[-1].get("url")
                        link_preview = {
                            "url": url,
                            "photo_url": photo_url
                        }
                    elif att_type == "doc":
                        doc = attach.get("doc", {})
                        ext = doc.get("ext")
                        direct_url = doc.get("url")
                        title = doc.get("title")

                        if ext == "gif" and direct_url:
                            gif_urls.append(direct_url)
                        elif direct_url:
                            doc_urls.append({
                                "url": direct_url,
                                "title": title,
                                "ext": ext
                            })
                    else:
                        skipped_types.append(att_type)
                        stats["skipped_types"].setdefault(att_type, 0)
                        stats["skipped_types"][att_type] += 1

                if skipped_types:
                    logger.info(f"üì¶ Post {post_url} skipped types: {', '.join(skipped_types)}")

                all_posts.append({
                    "text": post.get("text", "") if not is_repost else "",
                    "media_urls": media_urls,
                    "gif_urls": gif_urls,
                    "video_urls": video_urls,
                    "post_url": post_url,
                    "date": post_date,
                    "group_name": group_name,
                    "is_repost": is_repost,
                    "skipped_types": skipped_types,
                    "link_preview": link_preview,
                    "doc_urls": doc_urls,
                })
                
        except requests.exceptions.Timeout:
            logger.error(f"‚è∞ Timeout error for group {group}")
            stats["failed_groups"] += 1
            stats["group_errors"][group] = "Request timeout"
            continue
        except requests.exceptions.RequestException as e:
            logger.error(f"üåê Network error for group {group}: {str(e)}")
            stats["failed_groups"] += 1
            stats["group_errors"][group] = f"Network error: {str(e)}"
            continue
        except Exception as e:
            logger.error(f"‚ùå Unexpected error for group {group}: {str(e)}")
            stats["failed_groups"] += 1
            stats["group_errors"][group] = f"Unexpected error: {str(e)}"
            continue

    logger.success(f"üéØ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≥—Ä—É–ø–ø: {stats['processed_groups']}/{stats['total_groups']} | –ü–æ—Å—Ç–æ–≤: {stats['total_posts']} | –†–µ–ø–æ—Å—Ç–æ–≤: {stats['reposts']}")
    if stats["failed_groups"]:
        logger.warning(f"‚ö†Ô∏è –ì—Ä—É–ø–ø —Å –æ—à–∏–±–∫–∞–º–∏: {stats['failed_groups']}")
        # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª–∏ –æ—à–∏–±–æ–∫
        for group, error in stats["group_errors"].items():
            logger.warning(f"   ‚Ä¢ {group}: {error}")
    if stats["skipped_types"]:
        logger.info(f"üìå –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –≤–ª–æ–∂–µ–Ω–∏–π: {stats['skipped_types']}")

    return all_posts, stats


def prepare_vk_post_for_tg(
    posts: List[Dict],
    interleave_groups: bool = False
) -> Tuple[List[Dict[str, Union[str, List[str]]]], int]:
    if not isinstance(posts, list) or not posts:
        raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: –æ–∂–∏–¥–∞–µ—Ç—Å—è –Ω–µ–ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤")

    original_count = len(posts)
    
    # –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ –ø–æ—Å—Ç—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–ª–µ 'is_repost' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Ä–∞–≤–Ω–æ False
    posts = [post for post in posts if not post.get('is_repost', False)]
    reposts_skipped = original_count - len(posts)

    results = []

    grouped = {}
    for post in posts:
        group = post.get('group_name', 'default')
        grouped.setdefault(group, []).append(post)

    def prepare_post(post):
        text = post.get("text", "").replace("\\n", "\n").strip()
        text = remove_vk_links_but_keep_text(text)
        text = re.sub(r'https?://vk\.com[^\s]*', '', text)
        text = re.sub(r"\s*#\S+", "", text).strip()
        media_urls = post.get("media_urls", [])
        link_preview = post.get("link_preview", {})
        video_urls = post.get("video_urls", [])
        doc_urls = post.get("doc", {})
        gif_urls = post.get("gif_urls", [])
        group_name = post.get("group_name", "")
        original_post_url = post.get("post_url", "")
        post_date = post.get("date", "")

        # if not text and not media_urls and not link_preview:
        #     print(f"–ü—É—Å—Ç–æ–π –ø–æ—Å—Ç –æ—Ç {post.get('group_name')} —Å id {post.get('post_url', 'N/A')}")
        #     return None
        if not text and not media_urls and not link_preview and not video_urls:
            print(f"–ü—É—Å—Ç–æ–π –ø–æ—Å—Ç –æ—Ç {post.get('group_name')} —Å id {post.get('post_url', 'N/A')}")
            return None

        return {
            "text": text,
            "media_urls": media_urls,
            "video_urls": video_urls,
            "link_preview": link_preview,
            "gif_urls": gif_urls,
            "group_name": group_name,
            "original_post_url": original_post_url,
            "post_date": post_date,
        }

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å—Ç–æ–≤
    logger.info(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(grouped)} –≥—Ä—É–ø–ø –ø–æ—Å—Ç–æ–≤...")
    
    if interleave_groups:
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø
        max_len = max(len(group) for group in grouped.values())
        logger.info(f"üîÑ –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –ø–æ—Å—Ç—ã (–º–∞–∫—Å–∏–º—É–º {max_len} –ø–æ—Å—Ç–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã)")
        
        for i in range(max_len):
            for group_name, group in grouped.items():
                if i < len(group):
                    post = group[i]
                    prepared = prepare_post(post)
                    if prepared:
                        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å—Ç {i+1} –∏–∑ –≥—Ä—É–ø–ø—ã '{group_name}': {prepared.get('original_post_url')}")
                        results.append(prepared)
    else:
        # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
        for group_name, group_posts in grouped.items():
            logger.info(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—É '{group_name}' ({len(group_posts)} –ø–æ—Å—Ç–æ–≤)")
            for post in group_posts:
                prepared = prepare_post(post)
                if prepared:
                    logger.info(f"‚úÖ –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å—Ç –∏–∑ –≥—Ä—É–ø–ø—ã '{group_name}': {prepared.get('original_post_url')}")
                    results.append(prepared)

    return results, reposts_skipped
